\chapter{Artifact Development}

This chapter describes the development of the artifact, \texttt{ARKIVO.ART}, following the RQDD method introduced in the previous chapter.

\section{Plan Breakdown}

To answer a research question, one starts to plan what is required to answer the question, and then works backwards in incremental stages to where we are currently, leaving milestones (Miles) along the way, starting at 1 and going upwards as we get away from the goal. The analogy is that of a road, with each side-road marker showing how many miles we are away from a city, as we drive away from it.

We will start by trying to answer RQ1:

\vspace{0.5cm}

RQ1 - Can we determine which code-based HEN OBJKTs are networked?

\vspace{0.5cm}

We know that we can do this manually, by opening the OBJKT on a web browser and inspecting the network tab under the Developer Tools, to determine if there is any network activity to a domain or URL path that is outside of the OBJKT's IPFS URL. As an example we will use the artwork ``Adam'' by RaphaÃ«l de Courville\footnotemark[1], which we know to be a networked OBJKT.
As seen in \autoref{fig:devtools-net}, there is indeed a request to an external endpoint: \texttt{https\://api.hicdex.com/v1/graphql} initated by \texttt{sketch.js}, which is the p5js sketch that renders this artwork. The remaining 3 network requests are for local assets within the directory of the OBJKT, and hence not considered 'networking' as per our initial definition.

\footnotetext[1]{https://cache.teia.rocks/ipfs/QmbNtTu7k2E2UDYDQTyiVzV8rVbCU44hc9js1erKzeSogY/?\\creator=tz1hfuVWgcJ89ZE75ut9Qroi3y7GFJL5Lf2K\&viewer=\&objkt=230177}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{dev-tools-net.png}
    \caption[Developer Tools - Network Inspection]{Developer Tools - Network Inspection}
    \label{fig:devtools-net}
\end{figure}

Now that we established that we can identify a networked OBJKT manually, the next step is to do it programatically.

At this stage we can start to plan our Miles. The numbers will go up the further we get away from our goal.

Analysing the network traffic and determining if an OBJKT is networked is Mile 1.


For capturing the network traffic programatically we can use one of several headless browser automation solutions, which are normally used for automated website testing and web-scraping, like Playwright\footnotemark[2], Selenium\footnotemark[3], or Puppeteer\footnotemark[4]. For this work Playwright was selected. This is Mile 2.


\footnotetext[2]{https://playwright.dev/}
\footnotetext[3]{https://www.selenium.dev/}
\footnotetext[4]{https://pptr.dev/}

Although our Playwright instance could access the OBJKT asset directly from the Teia IPFS node, there are several reasons why it's not desirable to do so:

\begin{enumerate}
	\item introduces potential network latency which could affect the snapshot process adversely
	\item increases the bandwidth usage between teia and arkivo
	\item arkivo is supposed to archive these artworks anyway, including full copies of the assets
\end{enumerate}

For this reason the next step is to run our own IPFS node, so that we can fetch the OBJKTs locally, and pin them, which ensured it cannot be garbage collected by the node. This is Mile 3.

But this raises another question, which artworks to fetch? We need a way to discover and index all the OBJKTs that we are interested in investigating. This is Mile 4.

To discover the OBJKTs we can resort to existing indexers of the Tezos blockchain. There are 2 main options:

\begin{enumerate}
	\item TzKT API\footnotemark[5] - a generic indexer for the whole Tezos blockchain
	\item TezTok API\footnotemark[6] - an indexer specialised in Tezos NFTs
\end{enumerate}

\footnotetext[5]{https://tzkt.io/api}
\footnotetext[6]{https://www.teztok.com}

The TzKT API being generic, operates at a very low level, with endpoints that are closely related to the blockchain data, without abstraction layers.
TezTok, on the other hand, specialises in NFTs so it creates an abstraction layer that is easier to work with, since it is NFTs we are interested in discovering. For this reason TezTok was chosen.

Finally, we can move to the last stage, determining what search criteria to use when querying Teztok. This is Mile 5.

Here is a summary of the five Miles. We will write them in the reverse order, as now we are going back towards our goal.

\vspace{0.5cm}

\begin{table}[h!]
\centering
\begin{tabular}{|c|p{10cm}|}
\hline
\textbf{Mile \#} & \textbf{Description} \\ \hline
Mile 5 & Determine search criteria to use in Mile 4 \\ \hline
Mile 4 & Query TezTok to identify which artworks to fetch (discovery) \\ \hline
Mile 3 & Fetch and pin all OBJKTs of potential interest \\ \hline
Mile 2 & Run OBJKT in Playwright sandbox and capture traffic \\ \hline
Mile 1 & Analyse traffic and determine if the OBKJT is networked \\ \hline
\end{tabular}
\caption{The 5 Miles of Development for RQ1}
\end{table}

With these in place we draft an iteration plan. This can always be changed later but will give a rough idea of how many iterations are required.


\begin{table}[h!]
\centering
\begin{tabular}{|c|p{5cm}|}
\hline
\makecell{\textbf{Iteration \#}} & \makecell{\textbf{Miles to Goal}} \\ \hline
Iteration 1 & \makecell{Mile 5\\ Mile 4 } \\ \hline
Iteration 2 & \makecell{Mile 3}  \\ \hline
Iteration 3 & \makecell{Mile 2\\ Mile 1 } \\ \hline
\end{tabular}
\caption{Iteration Plan for RQ1}
\end{table}


\section {Iteration 1}
























\subsection{Risk Analysis}

Mention other risk analysis tools \cite{l2beatL2BEATStateLayer2024}

TODO:
* Asset storage (private servers, IPFS, Arweave)
* Libraries/Dependencies
* Networked Dependencies (type, blockchain indexer vs other APIs, public endpoints or private)
* Open source or not (obfuscated code, code running on private infra)








\section {Prototype 1 - genartdex}

\section{System Architecture}

boilerplate text, boilerplate text, boilerplate text, boilerplate text, boilerplate text, boilerplate text, boilerplate text, boilerplate text, boilerplate text, boilerplate text.


\subsection{Webpage metadata}

A common issue with most web pages, which can cause considerable annoyance to researchers who are working under tight deadlines, is the fact that they often lack the required metadata that is used by reference management tools, like Zotero, to automatically pull the metadata and populate the bibliographical record.

By default Arkivo suffered from the same problem, so attention was given to resolving this.
The \texttt{ArtifactController} now prepares a special data structure, \texttt{citationMetadata}, with all the required data fields, which is then used by the \texttt{header} section of the Edge template.
After experimenting with different metadata standards \cite{DevExposing_metadataZotero}\cite{zahidOpenGraphMeta2023} , a solution was found using a mixture of standards, where Zotero can pull most of the relevant fields related to an artifact, as shown in \autoref{fig:zotero-metadata-comparison}


\begin{lstlisting}[style=htmlCode,caption={Artifact page metadata}] 
<title>{{ citationMetadata.title}} - arkivo.art</title>
<meta property="og:title" content="{{ citationMetadata.title }}">
<meta property="og:type" content="website">
<meta property="dc.creator" content="{{ citationMetadata.author }}">
<meta property="article:published_time" content="{{ citationMetadata.date }}">
<meta property="og:description" content="{{ citationMetadata.description }}">
<meta property="og:site_name" content="arkivo.art">
\end{lstlisting}


\begin{figure}[H]
  \centering
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{page-metadata-before.png}
    \caption{Before header metadata}
    \label{fig:image1}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{page-metadata-after.png}
    \caption{After header metadata}
    \label{fig:image2}
  \end{subfigure}
  \caption{Artifact metadata pulled by Zotero}
  \label{fig:zotero-metadata-comparison}
\end{figure}



\subsection{Snapshots}

The first snapshot is important, as it is used for the initial classification of the OBJKT, as well as to establish a baseline record, against which future snapshots can be compared.
Therefore it should be taken immediately after discovery/ingestion.



\subsection{Encapsulation of internal IDs}

In normal circumstances the listing of a model, like Artifact, would include the unique ID of that model as stored in the database, for example: \texttt{/artifacts/45}

However this ID is unique to this instance of the application, and it can vary based on the timing of artifact discovery between other instances.
For this reason it is imperative that none of the internal IDs generated by the database are exposed to the user.

In order to achieve this, all URLs must be generated using only public identifiers which uniquely identify a model, in the case of an artifact: \texttt{blockchain /  smart contract / tokenId}.
This scheme closely follows existing proposals for cross-chain standardisation of identifiers \cite{herzogAssetTypeAsset2020}.

So what would be a URL:

\texttt{/artifacts/2065}

Becomes:

\texttt{/artifacts/tezos/KT1RJ6PbjHpwc3M5rw5s2Nbmefwbuwbdxton/230177}


However this URL is quite verbose. For convenience, since most of our initial artifacts are all on tezos, and on the HEN's OBJKT smart contract we can:

\begin{itemize}
    \item omit the blockchain identifier
    \item use an alias for the smart contract address
\end{itemize}

This strategy allows us to have shorter, but still unique URLs, which do not expose our internal DB IDs.

\texttt{/artifacts/HEN/230177}

Of course, the long-form URL should also work.

\subsection{Infrastructure}

A typical centralised application would be sensible to settle for state-of-the-art deployment platforms like Amazon ACS, DigitalOcean, Heroku, or Microsoft Azure. Not only do these commercial infrastructure solutions offer reliable up-time with fast Internet access, but also a wide range of geographically dispersed data centres across the world for maximum resiliency. It is theoretically possible to deploy a distributed and decentralised (control) solution across these platforms from an application point of view, however at the infrastructure level, the control would be centralised on a small number of industry players, which are an easy target for censorship requests from governments (ref needed).

\subsection{Smart Contracts}

boilerplate text, boilerplate text, boilerplate text, boilerplate text, boilerplate text, boilerplate text, boilerplate text, boilerplate text, boilerplate text, boilerplate text.

\subsection{Authentication}

todo: authentication vs authorisation vs access control

One of the main benefits of web3-style authentication is that it does not require any user credentials stored in the application's database. Since all web3 users have their public keys publicly registered on the blockchain, any user can identify themselves by \emph{connecting} their web3 wallet and signing a custom message provided by the application. This way the application can verify who the user is and that they are in possession of the private key, therefore validating their authentication credentials.

\section{Open Source}

Open sourcing this project was essencial because...

\subsection{Open Source License}

The choice of license...


The choice of license for this project was ... due to...



\section{Security: Detecting and Dealing with Malicious Code}

In this section I will talk about the security aspects of code-base artworks.